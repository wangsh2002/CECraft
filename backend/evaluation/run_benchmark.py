import asyncio
import os
import sys
import json
import time
from typing import List, Dict
from dotenv import load_dotenv
from openai import OpenAI

# Ensure backend is in path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from evaluation.rag_metrics import RAGEvaluator
from evaluation.business_value import BusinessValueEvaluator
# from evaluation.agent_perf import AgentPerformanceEvaluator # éœ€è¦å¼‚æ­¥ç¯å¢ƒï¼Œæš‚æ—¶åœ¨ main ä¸­å¤„ç†

# å¯¼å…¥ç³»ç»Ÿæ ¸å¿ƒåŠŸèƒ½
from app.services.tools.rag_retriever import search_and_rerank
from app.services.agent_workflow import run_agent_workflow, llm_service
from app.core.config import settings

# load_dotenv() # Config handles this

class BenchmarkRunner:
    def __init__(self):
        self.rag_evaluator = RAGEvaluator()
        self.biz_evaluator = BusinessValueEvaluator()
        self.client = OpenAI(api_key=settings.OPENAI_API_KEY, base_url=settings.OPENAI_API_BASE)
        self.model = settings.LLM_MODEL_PRO

    async def run_baseline(self, prompt: str, context: str = "") -> str:
        """
        Baseline: ç›´æ¥é—®å¤§æ¨¡å‹ (Zero-shot), ä¸æŸ¥åº“
        """
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç®€å†åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·æŒ‡ä»¤ä¿®æ”¹ç®€å†ã€‚"},
            {"role": "user", "content": f"ç”¨æˆ·æŒ‡ä»¤: {prompt}\nç®€å†å†…å®¹: {context}"}
        ]
        resp = self.client.chat.completions.create(
            model=self.model,
            messages=messages
        )
        return resp.choices[0].message.content

    async def run_system(self, prompt: str, context: dict) -> str:
        """
        System: æˆ‘ä»¬çš„ RAG + Agent ç³»ç»Ÿ
        """
        # è°ƒç”¨ agent_workflow
        result = await run_agent_workflow(prompt, context)
        # å‡è®¾ result è¿”å›ç»“æ„åŒ…å« 'content' (ä¿®æ”¹åçš„æ–‡æœ¬)
        # å¦‚æœ result æ˜¯ dict ä¸”åŒ…å« 'content'
        if isinstance(result, dict) and "content" in result:
            # å¦‚æœ content æ˜¯ Delta æ ¼å¼ (list/dict)ï¼Œè½¬ä¸º string ç”¨äºè¯„ä¼°
            return str(result["content"])
        return str(result)

    async def run_benchmark(self, test_dataset_path: str):
        print(f"Loading dataset from {test_dataset_path}...")
        with open(test_dataset_path, 'r') as f:
            dataset = json.load(f)

        results = {
            "rag_metrics": {"hit_rate": [], "faithfulness": [], "relevance": [], "mrr": []},
            "business_metrics": {"star_improvement": [], "jd_match_improvement": []},
            "agent_eval_metrics": {"pre_score": [], "post_score": [], "score_improvement": []},
            "performance_metrics": {"latency": [], "intent_accuracy": []},
            "win_rate": {"system_wins": 0, "baseline_wins": 0, "ties": 0}
        }

        print("Starting Benchmark...")
        
        for i, item in enumerate(dataset):
            print(f"\n--- Processing Case {i+1}/{len(dataset)} ---")
            question = item.get("question", "") or item.get("instruction", "")
            ground_truth = item.get("ground_truth", "")
            resume_context = item.get("resume_context", {}) # åŸå§‹ç®€å†ç‰‡æ®µ
            resume_text_raw = json.dumps(resume_context, ensure_ascii=False)
            target_type = item.get("type", "")
            
            # 1. RAG è¯„ä¼° (å¦‚æœæœ‰ ground_truth)
            # æ‰‹åŠ¨è§¦å‘æ£€ç´¢ä»¥è¯„ä¼° RAG è´¨é‡
            retrieved_docs = await asyncio.to_thread(search_and_rerank, question)
            
            # Debug: Check retrieval
            if not retrieved_docs:
                print(f"âš ï¸ [Warning] No docs retrieved for query: {question}")
                print("  -> Did you run 'python ingest_rag.py --source data/resumes_crawled'?")
            else:
                print(f"âœ… [Info] Retrieved {len(retrieved_docs)} docs. Top 1 snippet: {retrieved_docs[0].get('text', '')[:50]}...")

            contexts = [doc.get('text', '') for doc in retrieved_docs]
            
            if ground_truth:
                recall = self.rag_evaluator.evaluate_context_recall(question, contexts, ground_truth)
                mrr = self.rag_evaluator.calculate_mrr(question, contexts, ground_truth)
                results["rag_metrics"]["hit_rate"].append(recall)
                results["rag_metrics"]["mrr"].append(mrr)
                print(f"RAG Recall: {recall}, MRR: {mrr}")

            # 2. ç”Ÿæˆç»“æœå¯¹æ¯” (Baseline vs System) & æ€§èƒ½è¯„ä¼°
            baseline_output = await self.run_baseline(question, resume_text_raw)
            
            # è®¡æ—¶å¼€å§‹
            start_time = time.time()
            # è¿è¡Œç³»ç»Ÿå¹¶è·å–å®Œæ•´ç»“æœï¼ˆåŒ…å« Agent å†…éƒ¨è¯„ä¼°åˆ†æ•°ï¼‰
            system_result_full = await run_agent_workflow(question, resume_context)
            # è®¡æ—¶ç»“æŸ
            latency = time.time() - start_time
            results["performance_metrics"]["latency"].append(latency)
            print(f"Latency: {latency:.2f}s")
            
            # æ„å›¾è¯†åˆ«è¯„ä¼°
            predicted_intent = system_result_full.get("intent", "")
            is_intent_correct = False
            if target_type == "consult" and predicted_intent in ["research_consult", "chat"]:
                is_intent_correct = True
            elif target_type == "modify" and predicted_intent in ["modify", "research_modify"]:
                is_intent_correct = True
            
            results["performance_metrics"]["intent_accuracy"].append(1 if is_intent_correct else 0)
            print(f"Intent: {predicted_intent} (Expected: {target_type}) -> {'âœ…' if is_intent_correct else 'âŒ'}")

            # æå–å†…å®¹
            if isinstance(system_result_full.get("content"), (dict, list)):
                system_output = json.dumps(system_result_full["content"], ensure_ascii=False)
            else:
                system_output = str(system_result_full.get("content", ""))

            # 3. Agent å†…éƒ¨è¯„ä¼°åˆ†æ•°å¯¹æ¯” (Pre vs Post)
            # æ¨¡æ‹Ÿï¼šå‡è®¾åŸå§‹ç®€å†åˆ†æ•°ä¸º 60 (Baseline)ï¼ŒAgent è¯„ä¼°åçš„åˆ†æ•°ä¸º system_result_full.get('evaluation', {}).get('score')
            # å¦‚æœ Agent æµç¨‹ä¸­æ²¡æœ‰è¿”å› evaluationï¼Œæˆ‘ä»¬æ‰‹åŠ¨è°ƒç”¨ä¸€æ¬¡ Review Agent
            
            # Pre-Score: å¯¹åŸå§‹ç®€å†æ‰“åˆ†
            pre_eval = await llm_service.process_review_request(resume_text_raw)
            pre_score = pre_eval.get("score", 60)
            
            # Post-Score: å¯¹ä¿®æ”¹åç®€å†æ‰“åˆ†
            post_eval = await llm_service.process_review_request(system_output)
            post_score = post_eval.get("score", 0)
            
            results["agent_eval_metrics"]["pre_score"].append(pre_score)
            results["agent_eval_metrics"]["post_score"].append(post_score)
            results["agent_eval_metrics"]["score_improvement"].append(post_score - pre_score)
            print(f"Agent Eval Score: {pre_score} -> {post_score} (Diff: {post_score - pre_score})")

            # 4. RAG ç”Ÿæˆè´¨é‡è¯„ä¼° (Faithfulness & Relevance)
            # [æ”¹è¿›] å¯¹äº Modify ä»»åŠ¡ï¼ŒåŸå§‹ç®€å†ä¹Ÿæ˜¯åˆæ³•çš„ä¿¡æ¯æ¥æºï¼Œä¸åº”è¢«è§†ä¸ºå¹»è§‰
            # å°†åŸå§‹ç®€å†åŠ å…¥åˆ°ä¸Šä¸‹æ–‡åˆ—è¡¨ä¸­è¿›è¡Œè¯„ä¼°
            eval_contexts = contexts.copy()
            if resume_text_raw and resume_text_raw != "{}":
                eval_contexts.append(f"ã€ç”¨æˆ·åŸå§‹ç®€å†ä¿¡æ¯ã€‘: {resume_text_raw}")

            faithfulness = self.rag_evaluator.evaluate_faithfulness(question, eval_contexts, system_output)
            relevance = self.rag_evaluator.evaluate_answer_relevance(question, system_output)
            results["rag_metrics"]["faithfulness"].append(faithfulness)
            results["rag_metrics"]["relevance"].append(relevance)
            print(f"Faithfulness: {faithfulness}, Relevance: {relevance}")

            # 5. ä¸šåŠ¡ä»·å€¼è¯„ä¼° (STAR & JD Match)
            # å‡è®¾è¿™æ˜¯ä¸€ä¸ªç®€å†ä¿®æ”¹ä»»åŠ¡
            if "modify" in item.get("type", "modify"):
                star_eval = self.biz_evaluator.evaluate_star_compliance(resume_text_raw, system_output)
                results["business_metrics"]["star_improvement"].append(star_eval.get("improvement_percentage", 0))
                print(f"STAR Improvement: {star_eval.get('improvement_percentage', 0)}%")

            # 6. Win Rate (LLM-as-a-Judge)
            winner = self.judge_winner(question, baseline_output, system_output)
            if winner == "system":
                results["win_rate"]["system_wins"] += 1
            elif winner == "baseline":
                results["win_rate"]["baseline_wins"] += 1
            else:
                results["win_rate"]["ties"] += 1
            print(f"Winner: {winner}")

        self.print_report(results)

    def judge_winner(self, question: str, baseline_ans: str, system_ans: str) -> str:
        prompt = f"""
        è¯·å¯¹æ¯”ä¸¤ä¸ª AI åŠ©æ‰‹å¯¹ç”¨æˆ·æŒ‡ä»¤çš„æ‰§è¡Œç»“æœï¼Œé€‰å‡ºæ›´å¥½çš„ä¸€ä¸ªã€‚
        
        ç”¨æˆ·æŒ‡ä»¤: {question}
        
        ã€åŠ©æ‰‹ A (Baseline)ã€‘:
        {baseline_ans}
        
        ã€åŠ©æ‰‹ B (System)ã€‘:
        {system_ans}
        
        è¯·è¯„ä»·å“ªä¸ªæ›´å¥½ã€‚
        å¦‚æœ B æ˜æ˜¾æ›´å¥½ï¼ˆæ›´ä¸“ä¸šã€æ›´ç¬¦åˆæŒ‡ä»¤ã€ä½¿ç”¨äº†å¤–éƒ¨çŸ¥è¯†ï¼‰ï¼Œè¾“å‡º "system"ã€‚
        å¦‚æœ A æ›´å¥½ï¼Œè¾“å‡º "baseline"ã€‚
        å¦‚æœå·®ä¸å¤šï¼Œè¾“å‡º "tie"ã€‚
        ä»…è¾“å‡ºä¸€ä¸ªå•è¯ã€‚
        """
        try:
            resp = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}]
            )
            content = resp.choices[0].message.content.strip().lower()
            if "system" in content: return "system"
            if "baseline" in content: return "baseline"
            return "tie"
        except:
            return "tie"

    def print_report(self, results):
        total = len(results["rag_metrics"]["hit_rate"])
        if total == 0:
            print("No data to report.")
            return

        avg_hit_rate = sum(results["rag_metrics"]["hit_rate"]) / total
        avg_mrr = sum(results["rag_metrics"]["mrr"]) / total
        avg_faithfulness = sum(results["rag_metrics"]["faithfulness"]) / total
        avg_relevance = sum(results["rag_metrics"]["relevance"]) / total
        
        avg_star_imp = 0
        if results["business_metrics"]["star_improvement"]:
            avg_star_imp = sum(results["business_metrics"]["star_improvement"]) / len(results["business_metrics"]["star_improvement"])

        avg_pre_score = sum(results["agent_eval_metrics"]["pre_score"]) / len(results["agent_eval_metrics"]["pre_score"])
        avg_post_score = sum(results["agent_eval_metrics"]["post_score"]) / len(results["agent_eval_metrics"]["post_score"])
        avg_score_imp = sum(results["agent_eval_metrics"]["score_improvement"]) / len(results["agent_eval_metrics"]["score_improvement"])

        avg_latency = sum(results["performance_metrics"]["latency"]) / len(results["performance_metrics"]["latency"])
        avg_intent_acc = sum(results["performance_metrics"]["intent_accuracy"]) / len(results["performance_metrics"]["intent_accuracy"])

        wins = results["win_rate"]["system_wins"]
        ties = results["win_rate"]["ties"]
        losses = results["win_rate"]["baseline_wins"]
        total_battles = wins + ties + losses
        win_rate = (wins / total_battles) * 100 if total_battles > 0 else 0

        report = f"""
        ================================================
        ğŸ† CECraft System Benchmark Report
        ================================================
        
        1. RAG Core Metrics (æ£€ç´¢ä¸ç”Ÿæˆ)
        ------------------------------------------------
        - Context Recall (Hit Rate): {avg_hit_rate:.2f}
        - MRR (Mean Reciprocal Rank):{avg_mrr:.2f}
        - Faithfulness:              {avg_faithfulness:.2f}
        - Answer Relevance:          {avg_relevance:.2f}
        
        2. Agent Quality Metrics (ç®€å†è´¨é‡è¯„åˆ†)
        ------------------------------------------------
        - Pre-Optimization Score:    {avg_pre_score:.1f}
        - Post-Optimization Score:   {avg_post_score:.1f}
        - Average Score Improvement: +{avg_score_imp:.1f} pts
        
        3. Business Value (ä¸šåŠ¡æŒ‡æ ‡)
        ------------------------------------------------
        - Avg STAR Score Improvement: +{avg_star_imp:.1f}%
        
        4. Engineering Performance (å·¥ç¨‹æŒ‡æ ‡) <-- æ–°å¢ï¼
        ------------------------------------------------
        - Avg End-to-End Latency:    {avg_latency:.2f}s
        - Intent Recognition Acc:    {avg_intent_acc*100:.1f}%
        
        5. System vs Baseline (èƒœç‡)
        ------------------------------------------------
        - System Win Rate:           {win_rate:.1f}%
        - Record (W-L-T):            {wins}-{losses}-{ties}
        
        ================================================
        """
        print(report)
        # Save report
        with open("benchmark_report.txt", "w") as f:
            f.write(report)

if __name__ == "__main__":
    runner = BenchmarkRunner()
    # å‡è®¾æ•°æ®åœ¨ backend/data/benchmark_dataset.json
    dataset_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data", "benchmark_dataset.json")
    asyncio.run(runner.run_benchmark(dataset_path))
