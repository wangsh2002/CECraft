import os
import json
import sys
import asyncio
from typing import List, Dict
from dotenv import load_dotenv
from openai import OpenAI

# Setup path
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from app.services.tools.rag_retriever import search_and_rerank, retrieve_resume_examples
from app.core.config import settings

# load_dotenv() # Config handles this

class SimpleRAGEvaluator:
    def __init__(self):
        self.api_key = settings.OPENAI_API_KEY
        self.api_url = settings.OPENAI_API_BASE
        self.client = OpenAI(api_key=self.api_key, base_url=self.api_url)

    def evaluate_context_recall(self, question: str, contexts: List[str], ground_truth: str) -> float:
        """
        è¯„ä¼°ä¸Šä¸‹æ–‡å¬å›ç‡ï¼šæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ˜¯å¦åŒ…å«å›ç­”é—®é¢˜æ‰€éœ€çš„ä¿¡æ¯ï¼Ÿ
        """
        context_text = "\n".join(contexts)
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„è¯„ä¼°å‘˜ã€‚è¯·åˆ¤æ–­ä¸‹é¢çš„ã€æ£€ç´¢ä¸Šä¸‹æ–‡ã€‘æ˜¯å¦åŒ…å«äº†å›ç­”ã€é—®é¢˜ã€‘æ‰€éœ€çš„å…³é”®ä¿¡æ¯ï¼ˆå‚è€ƒã€æ ‡å‡†ç­”æ¡ˆã€‘ï¼‰ã€‚
        
        é—®é¢˜: {question}
        æ ‡å‡†ç­”æ¡ˆ: {ground_truth}
        æ£€ç´¢ä¸Šä¸‹æ–‡: {context_text}
        
        è¯·ä»…è¾“å‡ºä¸€ä¸ª 0 åˆ° 1 ä¹‹é—´çš„åˆ†æ•°ï¼Œè¡¨ç¤ºä¿¡æ¯çš„è¦†ç›–ç¨‹åº¦ã€‚
        1.0 è¡¨ç¤ºå®Œå…¨åŒ…å«æ‰€æœ‰å…³é”®ä¿¡æ¯ã€‚
        0.0 è¡¨ç¤ºå®Œå…¨ä¸ç›¸å…³ã€‚
        åªè¾“å‡ºæ•°å­—ï¼Œä¸è¦è§£é‡Šã€‚
        """
        try:
            resp = self.client.chat.completions.create(
                model="qwen-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
            score_str = resp.choices[0].message.content.strip()
            return float(score_str)
        except Exception as e:
            print(f"Error evaluating context recall: {e}")
            return 0.0

    def evaluate_answer_faithfulness(self, question: str, answer: str, contexts: List[str]) -> float:
        """
        è¯„ä¼°å›ç­”å¿ å®åº¦ï¼šç”Ÿæˆçš„å›ç­”æ˜¯å¦å®Œå…¨åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼Ÿ
        """
        context_text = "\n".join(contexts)
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„è¯„ä¼°å‘˜ã€‚è¯·åˆ¤æ–­ä¸‹é¢çš„ã€ç”Ÿæˆå›ç­”ã€‘æ˜¯å¦å®Œå…¨åŸºäºã€æ£€ç´¢ä¸Šä¸‹æ–‡ã€‘ç”Ÿæˆï¼Œè€Œæ²¡æœ‰å‡­ç©ºç¼–é€ ä¿¡æ¯ã€‚
        
        æ£€ç´¢ä¸Šä¸‹æ–‡: {context_text}
        ç”Ÿæˆå›ç­”: {answer}
        
        è¯·ä»…è¾“å‡ºä¸€ä¸ª 0 åˆ° 1 ä¹‹é—´çš„åˆ†æ•°ã€‚
        1.0 è¡¨ç¤ºå›ç­”å®Œå…¨ç”±ä¸Šä¸‹æ–‡æ”¯æŒã€‚
        0.0 è¡¨ç¤ºå›ç­”åŒ…å«å¤§é‡æœªåœ¨ä¸Šä¸‹æ–‡ä¸­å‡ºç°çš„ä¿¡æ¯ï¼ˆå¹»è§‰ï¼‰ã€‚
        åªè¾“å‡ºæ•°å­—ï¼Œä¸è¦è§£é‡Šã€‚
        """
        try:
            resp = self.client.chat.completions.create(
                model="qwen-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
            score_str = resp.choices[0].message.content.strip()
            return float(score_str)
        except Exception as e:
            print(f"Error evaluating faithfulness: {e}")
            return 0.0

    def run_evaluation(self, dataset_path: str):
        with open(dataset_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)

        print(f"ğŸš€ Starting RAG Evaluation on {len(dataset)} samples...\n")
        
        total_recall = 0.0
        total_faithfulness = 0.0
        
        for idx, item in enumerate(dataset):
            question = item['question']
            ground_truth = item['ground_truth']
            
            print(f"[{idx+1}/{len(dataset)}] Evaluating: {question}")
            
            # 1. Retrieve Contexts (using the new search_and_rerank function)
            try:
                docs = search_and_rerank(question, top_k=3)
                contexts = [d.get('text', '') or d.get('text_snippet', '') for d in docs]
            except Exception as e:
                print(f"  âŒ Retrieval failed: {e}")
                continue
                
            # 2. Generate Answer
            try:
                answer = retrieve_resume_examples(question, topk=3)
            except Exception as e:
                print(f"  âŒ Generation failed: {e}")
                continue

            # 3. Evaluate
            recall_score = self.evaluate_context_recall(question, contexts, ground_truth)
            faithfulness_score = self.evaluate_answer_faithfulness(question, answer, contexts)
            
            print(f"  - Context Recall: {recall_score}")
            print(f"  - Faithfulness:   {faithfulness_score}")
            print("-" * 30)
            
            total_recall += recall_score
            total_faithfulness += faithfulness_score

        avg_recall = total_recall / len(dataset) if dataset else 0
        avg_faithfulness = total_faithfulness / len(dataset) if dataset else 0
        
        print("\nğŸ“Š Evaluation Report")
        print("=" * 30)
        print(f"Average Context Recall:    {avg_recall:.2f}")
        print(f"Average Faithfulness:      {avg_faithfulness:.2f}")
        print("=" * 30)

if __name__ == "__main__":
    evaluator = SimpleRAGEvaluator()
    dataset_file = os.path.join(current_dir, "data", "eval_dataset.json")
    evaluator.run_evaluation(dataset_file)
