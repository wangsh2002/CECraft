import sys
import os
import asyncio
from datetime import datetime

# æ·»åŠ  backend ç›®å½•åˆ° sys.path
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

try:
    from app.services.tools.web_search import perform_web_search
except ImportError:
    # å¦‚æœç›´æ¥åœ¨ backend ç›®å½•ä¸‹è¿è¡Œï¼Œå¯èƒ½ä¸éœ€è¦è¿™ä¸€æ­¥ï¼Œä½†ä¸ºäº†ç¨³å¥æ€§ä¿ç•™
    sys.path.append(os.path.join(current_dir, ".."))
    from app.services.tools.web_search import perform_web_search

# ä¿å­˜ç›®å½•
SAVE_DIR = os.path.join(current_dir, "data", "resumes_crawled")
os.makedirs(SAVE_DIR, exist_ok=True)

# å®šä¹‰è¦æœé›†çš„ä¸»é¢˜åˆ—è¡¨
# åŒ…å«çƒ­é—¨æŠ€æœ¯å²—ä½éœ€æ±‚å’Œç®€å†èŒƒæ–‡
QUERIES = [
    "AI Agent å²—ä½èŒè´£ä¸æŠ€èƒ½è¦æ±‚",
    "é«˜çº§Pythonåç«¯å·¥ç¨‹å¸ˆ ç®€å†èŒƒæ–‡",
    "èµ„æ·±å‰ç«¯å¼€å‘å·¥ç¨‹å¸ˆ å²—ä½è¦æ±‚",
    "å¤§æ¨¡å‹ç®—æ³•å·¥ç¨‹å¸ˆ ç®€å†æ¨¡æ¿",
    "äº§å“ç»ç† æ ¸å¿ƒç«äº‰åŠ›ä¸ç®€å†æ’°å†™",
    "DevOps å·¥ç¨‹å¸ˆ æŠ€èƒ½å›¾è°±ä¸å²—ä½æè¿°",
    "å…¨æ ˆå·¥ç¨‹å¸ˆ ç®€å†é¡¹ç›®ç»éªŒå†™æ³•",
    "æ•°æ®åˆ†æå¸ˆ å²—ä½æŠ€èƒ½éœ€æ±‚"
]

async def crawl_and_save(query):
    print(f"ğŸ” [Crawl] æ­£åœ¨æœç´¢: {query} ...")
    try:
        # è°ƒç”¨æœç´¢å·¥å…·
        content = await perform_web_search(query)
        
        # ç”Ÿæˆæ–‡ä»¶å (æ›¿æ¢éæ³•å­—ç¬¦)
        safe_name = query.replace(" ", "_").replace("/", "_").replace("\\", "_")
        filename = f"{safe_name}.md"
        filepath = os.path.join(SAVE_DIR, filename)
        
        # æ·»åŠ å…ƒæ•°æ®å¤´ï¼Œæ–¹ä¾¿åç»­ RAG å¤„ç†
        file_content = f"""---
query: {query}
date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
source: web_search_tool
---

# {query}

{content}
"""
        
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(file_content)
            
        print(f"âœ… [Saved] å·²ä¿å­˜è‡³: {filepath}\n")
        
    except Exception as e:
        print(f"âŒ [Error] æœç´¢ '{query}' å¤±è´¥: {e}\n")

async def main():
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡æœé›†ç®€å†ä¸å²—ä½æ•°æ®ï¼Œå…± {len(QUERIES)} ä¸ªä»»åŠ¡")
    print(f"ğŸ“‚ ä¿å­˜è·¯å¾„: {SAVE_DIR}")
    print("-" * 50)
    
    for i, query in enumerate(QUERIES, 1):
        print(f"[{i}/{len(QUERIES)}] å¤„ç†ä»»åŠ¡: {query}")
        await crawl_and_save(query)
        # ç¨å¾®åœé¡¿ä¸€ä¸‹ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        await asyncio.sleep(2)
        
    print("-" * 50)
    print("ğŸ‰ æ‰€æœ‰æœé›†ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())
