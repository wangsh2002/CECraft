from __future__ import annotations

import os
import json
from typing import List, Optional, Dict, Any

from openai import OpenAI
from pymilvus import connections, Collection, utility
from app.core.config import settings

# å°è¯•å¯¼å…¥ dashscope ç”¨äº Rerank
try:
    import dashscope
except ImportError:
    dashscope = None

def _call_embedding_api(texts: List[str]) -> List[List[float]]:
    """
    è°ƒç”¨åµŒå…¥æ¨¡å‹APIï¼Œè·å–æ–‡æœ¬çš„åµŒå…¥è¡¨ç¤ºã€‚
    """
    texts = [t.replace("\n", " ") for t in texts]
    
    client = OpenAI(api_key=settings.OPENAI_API_KEY, 
                    base_url=settings.OPENAI_API_BASE)
    
    resp = client.embeddings.create(model=settings.EMBEDDING_MODEL_NAME, 
                                    input=texts, 
                                    encoding_format="float")
    
    data_items = sorted(resp.data, key=lambda x: x.index)
    embeddings = [item.embedding for item in data_items]
    return embeddings


def _ensure_milvus_connection():
    """
    è¿æ¥Milvus
    """
    try:
        connections.connect(host=settings.MILVUS_HOST, port=str(settings.MILVUS_PORT))
    except Exception as e:
        raise RuntimeError(f"æ— æ³•è¿æ¥åˆ°Milvus: {e}")


def _generate_answer_with_llm(query: str, context: str) -> str:
    """Use the OpenAI-compatible API to generate a final answer given query and retrieved context."""
    client = OpenAI(api_key=settings.OPENAI_API_KEY, base_url=settings.OPENAI_API_BASE)

    prompt = (
        "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©ç†ã€‚ä½¿ç”¨ä¸‹é¢çš„æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š\n\n"
        "ä¸Šä¸‹æ–‡:\n" + context + "\n\n"
        "é—®é¢˜: " + query + "\n\n"
        "è¯·ç»™å‡ºç®€æ˜ã€ä¸­æ–‡çš„æ‘˜è¦å¼å›ç­”ï¼Œä»…åŸºäºä¸Šé¢çš„æ£€ç´¢ä¸Šä¸‹æ–‡å›ç­”ï¼Œä¸è¦åˆ—å‡ºæˆ–æš´éœ²åŸå§‹ç‰‡æ®µçš„è·¯å¾„ã€chunk ç´¢å¼•æˆ–å…¶ä»–å…ƒæ•°æ®ã€‚ä¸¥ç¦å‡­ç©ºç¼–é€ äº‹å®ï¼›å¦‚æœä¸Šä¸‹æ–‡ä¸è¶³ä»¥å›ç­”ï¼Œè¯·æ˜ç¡®è¯´æ˜å¹¶ç»™å‡ºå»ºè®®ã€‚"
    )

    resp = client.chat.completions.create(model=settings.LLM_MODEL_NAME, 
                                          messages=[{"role": "user", "content": prompt}])
    
    # ç®€åŒ–æå–é€»è¾‘
    return resp.choices[0].message.content or ""


def _generate_sub_queries(query: str) -> List[str]:
    """
    ä½¿ç”¨ LLM ç”Ÿæˆç›¸å…³çš„å­æŸ¥è¯¢ï¼Œç”¨äºå¤šè·¯å¬å›
    """
    client = OpenAI(api_key=settings.OPENAI_API_KEY, base_url=settings.OPENAI_API_BASE)
    prompt = (
        f"ä½ æ˜¯ä¸€ä¸ªæœç´¢ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ '{query}'ï¼Œç”Ÿæˆ 3 ä¸ªç›¸å…³çš„æœç´¢æŸ¥è¯¢ï¼Œ"
        "ä»¥ä¾¿ä»ç®€å†æ•°æ®åº“æˆ–å²—ä½æè¿°ä¸­æ£€ç´¢åˆ°æ›´å…¨é¢çš„ä¿¡æ¯ã€‚\n"
        "è¯·ç›´æ¥è¾“å‡º 3 ä¸ªæŸ¥è¯¢ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦åŒ…å«ç¼–å·æˆ–é¢å¤–è§£é‡Šã€‚"
    )
    
    try:
        resp = client.chat.completions.create(
            model=settings.LLM_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}]
        )
        content = resp.choices[0].message.content
        if not content:
            return [query]
            
        sub_queries = [line.strip() for line in content.split('\n') if line.strip()]
        return [query] + sub_queries[:3] # åŒ…å«åŸå§‹æŸ¥è¯¢
    except Exception:
        return [query]


def _rerank_documents(query: str, docs: List[Dict[str, Any]], top_n: int) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨ DashScope Rerank æ¨¡å‹å¯¹æ–‡æ¡£è¿›è¡Œé‡æ’åº
    """
    if not docs:
        return []
        
    if not dashscope or not settings.DASHSCOPE_API_KEY:
        print("âš ï¸ [RAG] DashScope SDK not found or API Key missing. Skipping Rerank.")
        return docs[:top_n]

    try:
        dashscope.api_key = settings.DASHSCOPE_API_KEY
        # æå–æ–‡æœ¬åˆ—è¡¨
        doc_texts = [d.get("text", "") or d.get("text_snippet", "") for d in docs]
        
        resp = dashscope.TextReRank.call(
            model=settings.RERANK_MODEL_NAME,
            query=query,
            documents=doc_texts,
            top_n=top_n,
            return_documents=True
        )
        
        if resp.status_code == 200:
            reranked_docs = []
            for item in resp.output.results:
                original_idx = item.index
                doc = docs[original_idx]
                doc['rerank_score'] = item.relevance_score
                reranked_docs.append(doc)
            print(f"âœ… [RAG] Rerank successful. Top score: {reranked_docs[0]['rerank_score']}")
            return reranked_docs
        else:
            print(f"âš ï¸ [RAG] Rerank API failed: {resp.message}. Fallback to original order.")
            return docs[:top_n]
            
    except Exception as e:
        print(f"âš ï¸ [RAG] Rerank exception: {e}. Fallback to original order.")
        return docs[:top_n]


def search_and_rerank(query: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """
    æ‰§è¡Œå®Œæ•´çš„æ£€ç´¢æµç¨‹ï¼šQuery Expansion -> Vector Search -> Rerank
    """
    if not settings.DASHSCOPE_API_URL:
        raise RuntimeError("API URL is not set")

    _ensure_milvus_connection()

    collection_name = settings.RAG_COLLECTION or "md_collection"
    if not utility.has_collection(collection_name):
        raise RuntimeError(f"Milvus collection '{collection_name}' does not exist")

    coll = Collection(collection_name)
    coll.load()

    # 1. æŸ¥è¯¢æ‰©å±•
    queries = _generate_sub_queries(query)
    print(f"ğŸ” [RAG] Expanded queries: {queries}")

    # 2. æ‰¹é‡å‘é‡åŒ–
    embeddings = _call_embedding_api(queries)
    if not embeddings:
        raise RuntimeError("Failed to obtain embedding for query")
    
    # 3. å‘é‡æ£€ç´¢
    recall_k = top_k * 4 
    search_params = {"metric_type": "COSINE", "params": {"nprobe": 10}}
    limit_per_query = max(2, recall_k // len(queries) + 1)
    
    results = coll.search(embeddings, "embedding", param=search_params, limit=limit_per_query, output_fields=["metadata"])

    # 4. ç»“æœå»é‡ä¸åˆå¹¶
    unique_hits = {} 
    for hits in results:
        for hit in hits:
            meta_raw = hit.entity.get("metadata")
            if not meta_raw:
                continue
            try:
                meta = json.loads(meta_raw)
                key = (meta.get("source"), meta.get("chunk_index"))
                if 'text' not in meta:
                    meta['text'] = meta.get('text_snippet', '')
                
                if key not in unique_hits:
                    unique_hits[key] = {
                        "score": hit.score,
                        "meta": meta
                    }
                else:
                    if hit.score > unique_hits[key]["score"]:
                        unique_hits[key]["score"] = hit.score
            except:
                continue

    sorted_candidates = [item['meta'] for item in sorted(unique_hits.values(), key=lambda x: x["score"], reverse=True)]
    
    # 5. é‡æ’åº (Rerank)
    candidates_for_rerank = sorted_candidates[:50]
    final_docs = _rerank_documents(query, candidates_for_rerank, top_n=top_k)
    
    return final_docs


def retrieve_resume_examples(query: str, topk: Optional[int] = 5) -> str:
    """
    æŸ¥è¯¢RAGå‘é‡æ•°æ®åº“ï¼Œè·å–ç›¸å…³æ–‡æœ¬ç‰‡æ®µå¹¶ç”Ÿæˆå›ç­”
    """
    final_docs = search_and_rerank(query, top_k=topk)

    if not final_docs:
        return "æœªæ‰¾åˆ°åŒ¹é…çš„ç»“æœã€‚"

    out_items: List[str] = []
    for doc in final_docs:
        text_content = doc.get("text") or doc.get("text_snippet") or "(no content)"
        source = doc.get("source") or "(unknown)"
        chunk_index = doc.get("chunk_index")
        score = doc.get("rerank_score", 0)

        out = (
            f"source: {source} | chunk: {chunk_index} | score: {score}\n"
            f"{text_content}"
        )
        out_items.append(out)

    context = "\n\n---\n\n".join(out_items)

    answer = _generate_answer_with_llm(query=query, context=context)
    
    return answer

if __name__ == "__main__":
    q = "ä»€ä¹ˆæ˜¯æº¯æºå›¾ï¼Ÿ"
    try:
        print(retrieve_resume_examples(q))
    except Exception as e:
        print(f"æ£€ç´¢å¤±è´¥: {e}")
